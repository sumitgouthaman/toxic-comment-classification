{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic comment classification using LSTMs and pre-trained word-embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle recently ran a toxic comment classification [challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge). While I'm no ML expert, I tried my hand at attempting to make a decent submission.  \n",
    "\n",
    "**Disclaimer:** This notebook describes my best effort which garnered a score of 0.9790 (private) and 0.9805 (public), which put me roughly at the 59th percentile. So make sure to look at things below from this lens. This is quite far from a top solution, but it did require me to explore a lot of new concepts, which prompted me to blog this notebook anyways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics covered in this notebook:\n",
    "1. Pre-processing techniques for text data: tokenizing and limiting vocabulary.\n",
    "1. Using pre-trained word-embeddings.\n",
    "1. Using LSTM and CNN based models for comment classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries setup\n",
    "Getting all data-science related software setup correctly can be tricky. You can refer my [setup script](https://github.com/sumitgouthaman/workstation-setup) for details on my setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "We import all the libaries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sumit/Venv/mlpy3venv/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Python 2 / 3 compatibility\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "from datetime import datetime\n",
    "import io\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm  # To draw progress bars\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "# Keras\n",
    "from keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.utils.data_utils import get_file \n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peeking at the data\n",
    "The dataset for the Kaggle challenge is available at https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data.\n",
    "\n",
    "We load the train and test dataset as **Pandas** dataframes. We also have a sample submission file we can look at to ensure that the format of our output is correct.\n",
    "\n",
    "To make it convenient later, we store the output class names in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data\n",
    "# Then\n",
    "kaggle_train_dataset = pd.read_csv('train.csv')\n",
    "kaggle_test_dataset = pd.read_csv('test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "OUT_CLASSES = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first few rows of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_train_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now first few rows of the Kaggle test data.\n",
    "\n",
    "**Note:** We refer to the dataset used for evaluation by Kaggle as \"Kaggle test data\". This is to differentiate it from our \"test data\" that we siphon off from the Kaggle training data to validate how our training is going. We will only use the \"Kaggle test data\" towards the very end when we are ready to make a submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
       "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "4  00017695ad8997eb          I don't anonymously edit articles at all."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_test_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "1. Each comment in the dataset has an id associated with it. This probably doesn't matter for the training data. But for submitting the final predictions on the Kaggle test data, we would need to write the id and the predictions in the output file.  \n",
    "1. The Kaggle test data does not include the values for \"toxic\", \"obscene\", etc. Because we have to predict that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use the Pandas `describe` function to see stats about the dataframe columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095844</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>0.008805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.223931</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>0.216627</td>\n",
       "      <td>0.093420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  159571.000000  159571.000000  159571.000000  159571.000000   \n",
       "mean        0.095844       0.009996       0.052948       0.002996   \n",
       "std         0.294379       0.099477       0.223931       0.054650   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate  \n",
       "count  159571.000000  159571.000000  \n",
       "mean        0.049364       0.008805  \n",
       "std         0.216627       0.093420  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       0.000000  \n",
       "75%         0.000000       0.000000  \n",
       "max         1.000000       1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_train_dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the rows above are useless for this type of data. But the \"count\" row shows that we have 1/0 values for all the columns in all the rows. In other words, we don't need to deal with missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're at it, let's print out the sample submission to see what the expected output format is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
       "0  00001cee341fdb12    0.5           0.5      0.5     0.5     0.5   \n",
       "1  0000247867823ef7    0.5           0.5      0.5     0.5     0.5   \n",
       "2  00013b17ad220c46    0.5           0.5      0.5     0.5     0.5   \n",
       "3  00017563c3f7919a    0.5           0.5      0.5     0.5     0.5   \n",
       "4  00017695ad8997eb    0.5           0.5      0.5     0.5     0.5   \n",
       "\n",
       "   identity_hate  \n",
       "0            0.5  \n",
       "1            0.5  \n",
       "2            0.5  \n",
       "3            0.5  \n",
       "4            0.5  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's shuffle the training dataset.  \n",
    "\n",
    "We also separate the X and Y parts of the dataset. X contains the input (comments) and Y contains the 1/0 predictions for each of the output classes. `datasetX` and `datasetY` below are themselves Pandas dataframes and not numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_train_dataset = shuffle(kaggle_train_dataset)\n",
    "datasetX_comments = kaggle_train_dataset[['comment_text']]\n",
    "datasetY = kaggle_train_dataset[OUT_CLASSES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, let's ensure that `datasetX_comments` contains the comments column from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44725</th>\n",
       "      <td>Let me show you:  2 days ago the Executive Dir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153197</th>\n",
       "      <td>\"\\n :::Still the Golden Eagle is named as nati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157688</th>\n",
       "      <td>Dear 213.152.248.150:  Please stop vandalizing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123708</th>\n",
       "      <td>I don't understand your rationale here, rather...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75372</th>\n",
       "      <td>On the other hand, copious Hoa escaped with th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text\n",
       "44725   Let me show you:  2 days ago the Executive Dir...\n",
       "153197  \"\\n :::Still the Golden Eagle is named as nati...\n",
       "157688  Dear 213.152.248.150:  Please stop vandalizing...\n",
       "123708  I don't understand your rationale here, rather...\n",
       "75372   On the other hand, copious Hoa escaped with th..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetX_comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, lets ensure that `datasetY`contains just the output prediction columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44725</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153197</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157688</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123708</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75372</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "44725       0             0        0       0       0              0\n",
       "153197      0             0        0       0       0              0\n",
       "157688      0             0        0       0       0              0\n",
       "123708      0             0        0       0       0              0\n",
       "75372       0             0        0       0       0              0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetY.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most frameworks like Keras and Tensorflow expect numpy arrays, let's convert the Pandas dataframes to numpy arrays using the `as_matrix` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetX_comments = datasetX_comments.as_matrix().reshape((-1))\n",
    "datasetY = datasetY.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding stats about our input text\n",
    "There are some relevant stats about our input comments that would be useful to know:  \n",
    "1. What is the maximum length of any of the input comments?\n",
    "1. How many unique words are in the dataset?\n",
    "1. What does the frequency distribution of input words look like?\n",
    "\n",
    "We can use the `text_to_word_sequence` function from Keras that makes it easy to split text into individual words. It is not possible to get this splitting exactly right because text typically can have tons of punctuations that can complicate determining where words start and end. The `text_to_word_sequence` function however takes care of ignoring common punctuation characters.\n",
    "\n",
    "Let's do some processing to figure out the stats about our input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max comment word length: 1403\n",
      "Unique words: 210337\n"
     ]
    }
   ],
   "source": [
    "split_text = []\n",
    "max_comment_word_len = 0\n",
    "for c in datasetX_comments:\n",
    "    split_c = text_to_word_sequence(c)\n",
    "    max_comment_word_len = max(max_comment_word_len, len(split_c))\n",
    "    split_text.extend(split_c)\n",
    "\n",
    "unique_words_set = set(split_text)\n",
    "\n",
    "word_counts = {}\n",
    "for w in unique_words_set:\n",
    "    word_counts[w] = 0\n",
    "    \n",
    "for w in split_text:\n",
    "    word_counts[w] = word_counts[w] + 1\n",
    "\n",
    "print('Max comment word length: %d' % max_comment_word_len)\n",
    "print('Unique words: %d' % len(unique_words_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the distribution of word frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5faf758dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts_arr = sorted(list(word_counts.values()), reverse=True)\n",
    "plt.plot(counts_arr)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From that we see, there are more than 200K unique words. However, the long tail of words have very low frequencies. These are likely to be proper nouns.\n",
    "\n",
    "To simplify our model, we can decide on a cut off and only consider words that have frequency higher than a certain cut off. From the plot above, somewhere between 30K and 50K seems like a good value for out vocabulary size. Eg. If we choose 40K as our Vocab zie, in our model, we will ignore words that have frequency lower than the 40K th most common word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "When dealing with text, it is typical to assign a unique integer id to each word in the corpus. This makes it simpler to represent the data as we feed it into a model.\n",
    "\n",
    "Because this is such a common operation, Keras has built-in support for it. We need to **fit** the tokenizer object on our corpus so that it can assign a unique id to each word. We also initialize the tokenizer with a `num_words` parameter so that it knows how many of the top frequency words to take into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "Our model would expect each input row to be of the same dimension. This means once we decide on what the chosen input comment size is, we need to chop off some part of the input comments that are too long. We would also, need to **pad** the comments which are shorter.\n",
    "\n",
    "Keras has built in support for this using the `pad_sequences` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function that does both of these things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(datasetX_comments, max_words, max_len):\n",
    "    print ('Processing dataset, max_words: %d, max_length: %d' % (max_words, max_len))\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(datasetX_comments)\n",
    "\n",
    "    datasetX_tokenized = tokenizer.texts_to_sequences(datasetX_comments)\n",
    "    datasetX_padded = pad_sequences(datasetX_tokenized, maxlen=max_len)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens in dataset.' % len(word_index))\n",
    "    \n",
    "    return datasetX_padded, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets execute this function with a vocab of 20K and max length of 14K words to do a sanity check of the output. The output is the tokenized, padded version of the input comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset, max_words: 20000, max_length: 1400\n",
      "Found 210337 unique tokens in dataset.\n",
      "Let me show you:  2 days ago the Executive Director of the State Bar admitted that the State Bar was involved in acts of serious public corruption and it was covered by most major news outlets ( https://www.google.com/#tbm=nws&q;=state+bar+california+dunn ).  Who better than to expose the corruption than the executive director of the Bar itself?  It's the Bar who has admitted this.  Knowing this information benefits you and is well-within Wikipedia's rules (it's already been added to other Encyclopedia's and Legal Journals).  Ask yourself why these accounts below keep deleting this information without the required discussion.  \n",
      "\n",
      "Habitual Vandalizing of State Bar Article and Violations of 3 Revert Rule:\n",
      "[  0   0   0 ... 231 360 656]\n",
      "--------------------------------------------------\n",
      "\"\n",
      " :::Still the Golden Eagle is named as nationalcrest animal. It is not the crest animal, as you said the crest has no official species. So the remarks should be changed to \"\"the eagle is the national credt animal\"\" or something equally neutral. Also the Germans refer to Steinadler and Seeadler or just Adler, which then can be either. I am certain that most Germans will refer to the much larger white tailed eagle as \"\"their\"\" credt animal. But when there is no source for that, I concede. At least the incorrectness could be removed.\n",
      "\"\n",
      "[  0   0   0 ... 104  16 185]\n",
      "--------------------------------------------------\n",
      "DatasetX padded shape: (159571, 1400)\n"
     ]
    }
   ],
   "source": [
    "datasetX_padded, _ = process_dataset(datasetX_comments, 20000, 1400)\n",
    "for i in range(2):\n",
    "    print(datasetX_comments[i])\n",
    "    print(datasetX_padded[i])\n",
    "    print('-' * 50)\n",
    "print('DatasetX padded shape:', datasetX_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetX padded shape: (159571, 1400)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,   231,   360,   656],\n",
       "       [    0,     0,     0, ...,   104,    16,   185],\n",
       "       [    0,     0,     0, ...,    44,    16,   169],\n",
       "       [    0,     0,     0, ...,  4879,    12,     6],\n",
       "       [    0,     0,     0, ...,     3,     1, 12245]], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('DatasetX padded shape:', datasetX_padded.shape)\n",
    "datasetX_padded[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating a model\n",
    "\n",
    "We need a metric to evaluate our model. From the Kaggle competition page, the predictions will be scored in a very specific way:\n",
    "\n",
    "> Submissions are now evaluated on the mean column-wise ROC AUC. In other words, the score is the average of the individual AUCs of each predicted column.\n",
    "\n",
    "Ideally, we would like to evaluate the performance of our model using the same metric. Let's implement a Keras callback that prints the above custom metric as evaluated on a given test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMetricsCallback(Callback):\n",
    "    def __init__(self, testX, testY, freq=5):\n",
    "        self.testX = testX\n",
    "        self.testY = testY\n",
    "        self.freq = freq\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if ((epoch+1) % self.freq != 0):\n",
    "            return\n",
    "        print('\\n-- CUSTOM METRICS --')\n",
    "        testY_pred = self.model.predict(self.testX, verbose=1, batch_size=256)\n",
    "        roc_test = roc_auc_score(self.testY, testY_pred)\n",
    "        print('roc-auc (test): %.5f' % roc_test)\n",
    "        signifcant_actual = np.sum(np.any(self.testY > 0.5, axis=1))\n",
    "        signifcant_preds = np.sum(np.any(testY_pred > 0.5, axis=1))\n",
    "        print('Significant: %d (Actual) -- %d (Predictions)' % (signifcant_actual, signifcant_preds))\n",
    "        print('-- CUSTOM METRICS END --')\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would also like to print a sample of the predictions when the training ends. This serves a sanity check. For eg. we can verify that comments that are clearly toxic or obscene as marked as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintSamples(Callback):\n",
    "    def __init__(self, testX, testX_comment, testY):\n",
    "        self.testX = testX\n",
    "        self.testX_comment = testX_comment\n",
    "        self.testY = testY\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        testY_pred = self.model.predict(self.testX)\n",
    "        print('-' * 10, 'Samples ()', '-' * 10)\n",
    "        significant_example_idx = np.any(testY_pred >= 0.5, axis=1)\n",
    "        significant_examples = self.testX_comment[significant_example_idx]\n",
    "        significant_preds = testY_pred[significant_example_idx]\n",
    "        significant_actual = self.testY[significant_example_idx]\n",
    "        for i  in range(min(3, len(significant_examples))):\n",
    "            print('Example: ', significant_examples[i])\n",
    "            print('class\\t\\tActual\\tPred')\n",
    "            for ic, c in enumerate(OUT_CLASSES):\n",
    "                print('%s\\t\\t%.3f\\t%.3f' % (c, significant_actual[i][ic], significant_preds[i][ic]))\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models involving text and LSTMs are very time consuming to train. We would like to checkpoint the model every few epochs.\n",
    "\n",
    "Let's implement a helper method the returns a checkpointing callback for a given model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoint_callback(model):\n",
    "    dir_name = model.name\n",
    "    if not os.path.isdir(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "    return ModelCheckpoint(\n",
    "        dir_name + '/weights.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "        monitor='val_loss',\n",
    "        period=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we implement a callback the would use the model to make predictions on the \"Kaggle test dataset\" and write the prediction to a file in the format Kaggle expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KaggleOutputCallback(Callback):\n",
    "    def __init__(self, tokenizer, kaggle_test_dataset, max_len, freq=5):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.kaggle_test_dataset = kaggle_test_dataset\n",
    "        self.max_len = max_len\n",
    "        self.freq = freq\n",
    "    \n",
    "    def output(self, model, suffix=''):\n",
    "        kaggle_testset_ids = self.kaggle_test_dataset[['id']].as_matrix().reshape((-1))\n",
    "        kaggle_testset_comments = self.kaggle_test_dataset[['comment_text']].as_matrix().reshape((-1))\n",
    "\n",
    "        kaggle_testX_tokenized = self.tokenizer.texts_to_sequences(kaggle_testset_comments)\n",
    "        kaggle_testX_padded = pad_sequences(kaggle_testX_tokenized, maxlen=self.max_len)\n",
    "\n",
    "        kaggle_testY_pred = model.predict(kaggle_testX_padded, verbose=1, batch_size=256)\n",
    "\n",
    "        dir_name = model.name\n",
    "        if not os.path.isdir(dir_name):\n",
    "            os.makedirs(dir_name)\n",
    "        \n",
    "        fname = dir_name + '/output' + suffix + '.csv'\n",
    "        with open(fname, 'w') as out:\n",
    "            out.write('id,toxic,severe_toxic,obscene,threat,insult,identity_hate\\n')\n",
    "            for i, cid in enumerate(kaggle_testset_ids):\n",
    "                out_line = '%s,%s\\n' %(cid, ','.join(map(str, kaggle_testY_pred[i])))\n",
    "                out.write(out_line)\n",
    "        print('Wrote to file: ', fname)\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if ((epoch+1) % self.freq != 0):\n",
    "            return\n",
    "        self.output(self.model, '-' + str(epoch))\n",
    "        return\n",
    "    \n",
    "    def on_train_end(self, logs={}):\n",
    "        self.output(self.model, '-end')\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings\n",
    "When it comes to models dealing with text data, word embeddings are a fantastic option.\n",
    "\n",
    "There are others on the internet who have done a great job explaining work embeddings, so I\n",
    "wont bother. Here's a few great videos: [[1]](https://www.youtube.com/watch?v=Eku_pbZ3-Mw), [[2]](https://www.youtube.com/watch?v=5PL0TmQhItY).\n",
    "\n",
    "Here's the gist:\n",
    "- When we represent each unique word in our corpus with an integer id, there is no relation between the ids assigned to similar words. Eg. The id for \"King\" and \"Queen\" would have no relation to each other.\n",
    "- Instead, we use pre-trained word embeddings. These embeddings represent each word with a vector value. And these word vectors have interesting properties. For eg. If we subtract the vector for \"Queen\" from the vector for \"King\", we would get a vector that has very high cosine similarity to the vector formed by subtracting \"Woman\" from \"Man\".\n",
    "- Using word vectors, our model can understand that \"good job\" and \"great job\" have similar meanings even if the actual input does not have \"great job\" anywhere.\n",
    "\n",
    "#### Word embeddings in Keras\n",
    "I did a simple Google search and found a good example of using pre-trainined embeddings in Keras: [[Link]](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html).\n",
    "\n",
    "There are many options available when it comes to word embeddings: word2vec, Glove are names of pre-trained word embedding vectors that others have trained and made available.\n",
    "\n",
    "Lets write a function that implements a Keras embedding layer for us given some parameters we care about. **Note:** this assumes that we have downloaded the Glove embedding values from http://nlp.stanford.edu/data/glove.6B.zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "\n",
    "def get_embedding_layer(tokenizer, embedding_dims, max_words, max_len, trainable=False):\n",
    "    embeddings_index = {}\n",
    "    with open(os.path.join(GLOVE_DIR, 'glove.6B.%dd.txt' % embedding_dims)) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    # prepare embedding matrix\n",
    "    word_index = tokenizer.word_index\n",
    "    num_words = min(max_words, len(word_index))\n",
    "    embedding_matrix = np.zeros((num_words, embedding_dims))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    # load pre-trained word embeddings into an Embedding layer\n",
    "    # note that we set trainable = False so as to keep the embeddings fixed\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                embedding_dims,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_len,\n",
    "                                trainable=trainable)  \n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition\n",
    "I tried many model architectures. Eventually, I arrived at one that gave me decent results. This is the only architecture I have included in this notebook.\n",
    "\n",
    "This is a **multi-class, multi-label** classification problem. This means, for each input instance, multiple output classes could be possible at the same time. A comment could be both \"toxic\" and \"obscene\" at the same time.\n",
    "\n",
    "Now I did some more research about multi-label classification. I found this blog post that was very helpful: [[Link]](https://www.depends-on-the-definition.com/guide-to-multi-label-classification-with-neural-networks/).\n",
    "\n",
    "Few things to  keep in mind:  \n",
    "1. We can arrange for the output size of our final fully-connected layer to be same as the number of output classes we are trying to predict.  \n",
    "1. We should use the **\"sigmoid\"** activation instead of \"softmax\" activation we use in a classification problem. This is because in case of sigmoid, the value for  each class is independent of the others, while this is not the case for softmax.\n",
    "1. We should use **\"binary-crossentropy\"** as the loss instead of \"categorical_crossentropy\" we typically use in a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset, max_words: 40000, max_length: 1400\n",
      "Found 210337 unique tokens in dataset.\n",
      "Found 400000 word vectors.\n",
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/30\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9766-- CUSTOM METRICS --\n",
      "7979/7979 [==============================] - 14s 2ms/step\n",
      "roc-auc (test): 0.97602\n",
      "Significant: 775 (Actual) -- 638 (Predictions)\n",
      "-- CUSTOM METRICS END --\n",
      "153164/153164 [==============================] - 256s 2ms/step\n",
      "Wrote to file:  1_BiLstm_128-1_conv_128-Avg_Max_Pool-dense_128-200_dim_emb-40000_vocab/output-0.csv\n",
      "151592/151592 [==============================] - 2553s 17ms/step - loss: 0.0696 - acc: 0.9766 - val_loss: 0.0607 - val_acc: 0.9820\n",
      "Epoch 2/30\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9806-- CUSTOM METRICS --\n",
      "7979/7979 [==============================] - 14s 2ms/step\n",
      "roc-auc (test): 0.98058\n",
      "Significant: 775 (Actual) -- 659 (Predictions)\n",
      "-- CUSTOM METRICS END --\n",
      "153164/153164 [==============================] - 256s 2ms/step\n",
      "Wrote to file:  1_BiLstm_128-1_conv_128-Avg_Max_Pool-dense_128-200_dim_emb-40000_vocab/output-1.csv\n",
      "151592/151592 [==============================] - 2552s 17ms/step - loss: 0.0539 - acc: 0.9806 - val_loss: 0.0567 - val_acc: 0.9827\n",
      "Epoch 3/30\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9815-- CUSTOM METRICS --\n",
      "7979/7979 [==============================] - 14s 2ms/step\n",
      "roc-auc (test): 0.98282\n",
      "Significant: 775 (Actual) -- 615 (Predictions)\n",
      "-- CUSTOM METRICS END --\n",
      "153164/153164 [==============================] - 256s 2ms/step\n",
      "Wrote to file:  1_BiLstm_128-1_conv_128-Avg_Max_Pool-dense_128-200_dim_emb-40000_vocab/output-2.csv\n",
      "151592/151592 [==============================] - 2553s 17ms/step - loss: 0.0506 - acc: 0.9815 - val_loss: 0.0554 - val_acc: 0.9834\n",
      "Epoch 4/30\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9821-- CUSTOM METRICS --\n",
      "7979/7979 [==============================] - 14s 2ms/step\n",
      "roc-auc (test): 0.98414\n",
      "Significant: 775 (Actual) -- 611 (Predictions)\n",
      "-- CUSTOM METRICS END --\n",
      "153164/153164 [==============================] - 256s 2ms/step\n",
      "Wrote to file:  1_BiLstm_128-1_conv_128-Avg_Max_Pool-dense_128-200_dim_emb-40000_vocab/output-3.csv\n",
      "151592/151592 [==============================] - 2554s 17ms/step - loss: 0.0480 - acc: 0.9821 - val_loss: 0.0547 - val_acc: 0.9839\n",
      "Epoch 5/30\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9825-- CUSTOM METRICS --\n",
      "7979/7979 [==============================] - 14s 2ms/step\n",
      "roc-auc (test): 0.98519\n",
      "Significant: 775 (Actual) -- 655 (Predictions)\n",
      "-- CUSTOM METRICS END --\n",
      "153164/153164 [==============================] - 257s 2ms/step\n",
      "Wrote to file:  1_BiLstm_128-1_conv_128-Avg_Max_Pool-dense_128-200_dim_emb-40000_vocab/output-4.csv\n",
      "151592/151592 [==============================] - 2553s 17ms/step - loss: 0.0461 - acc: 0.9825 - val_loss: 0.0551 - val_acc: 0.9833\n",
      "Epoch 6/30\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9830-- CUSTOM METRICS --\n",
      "7979/7979 [==============================] - 14s 2ms/step\n",
      "roc-auc (test): 0.98594\n",
      "Significant: 775 (Actual) -- 624 (Predictions)\n",
      "-- CUSTOM METRICS END --\n",
      "153164/153164 [==============================] - 257s 2ms/step\n",
      "Wrote to file:  1_BiLstm_128-1_conv_128-Avg_Max_Pool-dense_128-200_dim_emb-40000_vocab/output-5.csv\n",
      "151592/151592 [==============================] - 2553s 17ms/step - loss: 0.0446 - acc: 0.9830 - val_loss: 0.0466 - val_acc: 0.9843\n",
      "Epoch 7/30\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9833-- CUSTOM METRICS --\n",
      "7979/7979 [==============================] - 14s 2ms/step\n",
      "roc-auc (test): 0.98617\n",
      "Significant: 775 (Actual) -- 733 (Predictions)\n",
      "-- CUSTOM METRICS END --\n",
      "153164/153164 [==============================] - 256s 2ms/step\n",
      "Wrote to file:  1_BiLstm_128-1_conv_128-Avg_Max_Pool-dense_128-200_dim_emb-40000_vocab/output-6.csv\n",
      "151592/151592 [==============================] - 2552s 17ms/step - loss: 0.0433 - acc: 0.9833 - val_loss: 0.0566 - val_acc: 0.9833\n",
      "Epoch 8/30\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9838-- CUSTOM METRICS --\n",
      "7979/7979 [==============================] - 14s 2ms/step\n",
      "roc-auc (test): 0.98630\n",
      "Significant: 775 (Actual) -- 700 (Predictions)\n",
      "-- CUSTOM METRICS END --\n",
      "153164/153164 [==============================] - 256s 2ms/step\n",
      "Wrote to file:  1_BiLstm_128-1_conv_128-Avg_Max_Pool-dense_128-200_dim_emb-40000_vocab/output-7.csv\n",
      "151592/151592 [==============================] - 2553s 17ms/step - loss: 0.0421 - acc: 0.9838 - val_loss: 0.0512 - val_acc: 0.9834\n",
      "Epoch 9/30\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9841-- CUSTOM METRICS --\n",
      "7979/7979 [==============================] - 14s 2ms/step\n",
      "roc-auc (test): 0.98623\n",
      "Significant: 775 (Actual) -- 658 (Predictions)\n",
      "-- CUSTOM METRICS END --\n",
      "153164/153164 [==============================] - 257s 2ms/step\n",
      "Wrote to file:  1_BiLstm_128-1_conv_128-Avg_Max_Pool-dense_128-200_dim_emb-40000_vocab/output-8.csv\n",
      "151592/151592 [==============================] - 2554s 17ms/step - loss: 0.0407 - acc: 0.9841 - val_loss: 0.0529 - val_acc: 0.9842\n",
      "Epoch 10/30\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9844-- CUSTOM METRICS --\n",
      "7979/7979 [==============================] - 14s 2ms/step\n",
      "roc-auc (test): 0.98554\n",
      "Significant: 775 (Actual) -- 652 (Predictions)\n",
      "-- CUSTOM METRICS END --\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "153164/153164 [==============================] - 256s 2ms/step\n",
      "Wrote to file:  1_BiLstm_128-1_conv_128-Avg_Max_Pool-dense_128-200_dim_emb-40000_vocab/output-9.csv\n",
      "151592/151592 [==============================] - 2553s 17ms/step - loss: 0.0399 - acc: 0.9844 - val_loss: 0.0471 - val_acc: 0.9841\n",
      "Epoch 11/30\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9852-- CUSTOM METRICS --\n",
      "7979/7979 [==============================] - 14s 2ms/step\n",
      "roc-auc (test): 0.98593\n",
      "Significant: 775 (Actual) -- 721 (Predictions)\n",
      "-- CUSTOM METRICS END --\n",
      "153164/153164 [==============================] - 256s 2ms/step\n",
      "Wrote to file:  1_BiLstm_128-1_conv_128-Avg_Max_Pool-dense_128-200_dim_emb-40000_vocab/output-10.csv\n",
      "151592/151592 [==============================] - 2553s 17ms/step - loss: 0.0373 - acc: 0.9852 - val_loss: 0.0478 - val_acc: 0.9834\n",
      "Epoch 12/30\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9855-- CUSTOM METRICS --\n",
      "7979/7979 [==============================] - 14s 2ms/step\n",
      "roc-auc (test): 0.98634\n",
      "Significant: 775 (Actual) -- 735 (Predictions)\n",
      "-- CUSTOM METRICS END --\n",
      "153164/153164 [==============================] - 256s 2ms/step\n",
      "Wrote to file:  1_BiLstm_128-1_conv_128-Avg_Max_Pool-dense_128-200_dim_emb-40000_vocab/output-11.csv\n",
      "151592/151592 [==============================] - 2555s 17ms/step - loss: 0.0363 - acc: 0.9855 - val_loss: 0.0482 - val_acc: 0.9836\n",
      "Epoch 13/30\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9858-- CUSTOM METRICS --\n",
      "7979/7979 [==============================] - 14s 2ms/step\n",
      "roc-auc (test): 0.98568\n",
      "Significant: 775 (Actual) -- 674 (Predictions)\n",
      "-- CUSTOM METRICS END --\n",
      "153164/153164 [==============================] - 256s 2ms/step\n",
      "Wrote to file:  1_BiLstm_128-1_conv_128-Avg_Max_Pool-dense_128-200_dim_emb-40000_vocab/output-12.csv\n",
      "151592/151592 [==============================] - 2552s 17ms/step - loss: 0.0355 - acc: 0.9858 - val_loss: 0.0464 - val_acc: 0.9844\n",
      "Epoch 14/30\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9860-- CUSTOM METRICS --\n",
      "7979/7979 [==============================] - 14s 2ms/step\n",
      "roc-auc (test): 0.98543\n",
      "Significant: 775 (Actual) -- 731 (Predictions)\n",
      "-- CUSTOM METRICS END --\n",
      "153164/153164 [==============================] - 256s 2ms/step\n",
      "Wrote to file:  1_BiLstm_128-1_conv_128-Avg_Max_Pool-dense_128-200_dim_emb-40000_vocab/output-13.csv\n",
      "151592/151592 [==============================] - 2554s 17ms/step - loss: 0.0348 - acc: 0.9860 - val_loss: 0.0456 - val_acc: 0.9835\n",
      "Epoch 15/30\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9863-- CUSTOM METRICS --\n",
      "7979/7979 [==============================] - 14s 2ms/step\n",
      "roc-auc (test): 0.98453\n",
      "Significant: 775 (Actual) -- 750 (Predictions)\n",
      "-- CUSTOM METRICS END --\n",
      "153164/153164 [==============================] - 256s 2ms/step\n",
      "Wrote to file:  1_BiLstm_128-1_conv_128-Avg_Max_Pool-dense_128-200_dim_emb-40000_vocab/output-14.csv\n",
      "151592/151592 [==============================] - 2554s 17ms/step - loss: 0.0340 - acc: 0.9863 - val_loss: 0.0472 - val_acc: 0.9830\n",
      "Epoch 16/30\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9864-- CUSTOM METRICS --\n",
      "7979/7979 [==============================] - 14s 2ms/step\n",
      "roc-auc (test): 0.98452\n",
      "Significant: 775 (Actual) -- 698 (Predictions)\n",
      "-- CUSTOM METRICS END --\n",
      "153164/153164 [==============================] - 257s 2ms/step\n",
      "Wrote to file:  1_BiLstm_128-1_conv_128-Avg_Max_Pool-dense_128-200_dim_emb-40000_vocab/output-15.csv\n",
      "151592/151592 [==============================] - 2553s 17ms/step - loss: 0.0335 - acc: 0.9864 - val_loss: 0.0465 - val_acc: 0.9835\n",
      "Epoch 17/30\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9867-- CUSTOM METRICS --\n",
      "7979/7979 [==============================] - 14s 2ms/step\n",
      "roc-auc (test): 0.98581\n",
      "Significant: 775 (Actual) -- 694 (Predictions)\n",
      "-- CUSTOM METRICS END --\n",
      "153164/153164 [==============================] - 256s 2ms/step\n",
      "Wrote to file:  1_BiLstm_128-1_conv_128-Avg_Max_Pool-dense_128-200_dim_emb-40000_vocab/output-16.csv\n",
      "151592/151592 [==============================] - 2552s 17ms/step - loss: 0.0328 - acc: 0.9867 - val_loss: 0.0471 - val_acc: 0.9834\n",
      "Epoch 18/30\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9868-- CUSTOM METRICS --\n",
      "7979/7979 [==============================] - 14s 2ms/step\n",
      "roc-auc (test): 0.98440\n",
      "Significant: 775 (Actual) -- 695 (Predictions)\n",
      "-- CUSTOM METRICS END --\n",
      "153164/153164 [==============================] - 256s 2ms/step\n",
      "Wrote to file:  1_BiLstm_128-1_conv_128-Avg_Max_Pool-dense_128-200_dim_emb-40000_vocab/output-17.csv\n",
      "151592/151592 [==============================] - 2551s 17ms/step - loss: 0.0324 - acc: 0.9868 - val_loss: 0.0451 - val_acc: 0.9836\n",
      "Epoch 19/30\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9869-- CUSTOM METRICS --\n",
      "7979/7979 [==============================] - 14s 2ms/step\n",
      "roc-auc (test): 0.98490\n",
      "Significant: 775 (Actual) -- 723 (Predictions)\n",
      "-- CUSTOM METRICS END --\n",
      "153164/153164 [==============================] - 256s 2ms/step\n",
      "Wrote to file:  1_BiLstm_128-1_conv_128-Avg_Max_Pool-dense_128-200_dim_emb-40000_vocab/output-18.csv\n",
      "151592/151592 [==============================] - 2554s 17ms/step - loss: 0.0319 - acc: 0.9869 - val_loss: 0.0455 - val_acc: 0.9831\n",
      "Epoch 20/30\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9870-- CUSTOM METRICS --\n",
      "7979/7979 [==============================] - 14s 2ms/step\n",
      "roc-auc (test): 0.98527\n",
      "Significant: 775 (Actual) -- 756 (Predictions)\n",
      "-- CUSTOM METRICS END --\n",
      "153164/153164 [==============================] - 256s 2ms/step\n",
      "Wrote to file:  1_BiLstm_128-1_conv_128-Avg_Max_Pool-dense_128-200_dim_emb-40000_vocab/output-19.csv\n",
      "151592/151592 [==============================] - 2553s 17ms/step - loss: 0.0314 - acc: 0.9870 - val_loss: 0.0474 - val_acc: 0.9831\n",
      "Epoch 21/30\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9873-- CUSTOM METRICS --\n",
      "7979/7979 [==============================] - 14s 2ms/step\n",
      "roc-auc (test): 0.98403\n",
      "Significant: 775 (Actual) -- 699 (Predictions)\n",
      "-- CUSTOM METRICS END --\n",
      "153164/153164 [==============================] - 256s 2ms/step\n",
      "Wrote to file:  1_BiLstm_128-1_conv_128-Avg_Max_Pool-dense_128-200_dim_emb-40000_vocab/output-20.csv\n",
      "151592/151592 [==============================] - 2552s 17ms/step - loss: 0.0312 - acc: 0.9873 - val_loss: 0.0448 - val_acc: 0.9835\n",
      "Epoch 22/30\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9874-- CUSTOM METRICS --\n",
      "7979/7979 [==============================] - 14s 2ms/step\n",
      "roc-auc (test): 0.98394\n",
      "Significant: 775 (Actual) -- 671 (Predictions)\n",
      "-- CUSTOM METRICS END --\n",
      "153164/153164 [==============================] - 256s 2ms/step\n",
      "Wrote to file:  1_BiLstm_128-1_conv_128-Avg_Max_Pool-dense_128-200_dim_emb-40000_vocab/output-21.csv\n",
      "151592/151592 [==============================] - 2551s 17ms/step - loss: 0.0305 - acc: 0.9874 - val_loss: 0.0449 - val_acc: 0.9832\n",
      "Epoch 23/30\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9875-- CUSTOM METRICS --\n",
      "7979/7979 [==============================] - 14s 2ms/step\n",
      "roc-auc (test): 0.98385\n",
      "Significant: 775 (Actual) -- 711 (Predictions)\n",
      "-- CUSTOM METRICS END --\n",
      "153164/153164 [==============================] - 256s 2ms/step\n",
      "Wrote to file:  1_BiLstm_128-1_conv_128-Avg_Max_Pool-dense_128-200_dim_emb-40000_vocab/output-22.csv\n",
      "151592/151592 [==============================] - 2554s 17ms/step - loss: 0.0300 - acc: 0.9875 - val_loss: 0.0448 - val_acc: 0.9837\n",
      "Epoch 24/30\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9878-- CUSTOM METRICS --\n",
      "7979/7979 [==============================] - 14s 2ms/step\n",
      "roc-auc (test): 0.98324\n",
      "Significant: 775 (Actual) -- 737 (Predictions)\n",
      "-- CUSTOM METRICS END --\n",
      "153164/153164 [==============================] - 256s 2ms/step\n",
      "Wrote to file:  1_BiLstm_128-1_conv_128-Avg_Max_Pool-dense_128-200_dim_emb-40000_vocab/output-23.csv\n",
      "151592/151592 [==============================] - 2553s 17ms/step - loss: 0.0296 - acc: 0.9878 - val_loss: 0.0458 - val_acc: 0.9833\n",
      "Epoch 25/30\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9872-- CUSTOM METRICS --\n",
      "7979/7979 [==============================] - 14s 2ms/step\n",
      "roc-auc (test): 0.98100\n",
      "Significant: 775 (Actual) -- 812 (Predictions)\n",
      "-- CUSTOM METRICS END --\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "153164/153164 [==============================] - 256s 2ms/step\n",
      "Wrote to file:  1_BiLstm_128-1_conv_128-Avg_Max_Pool-dense_128-200_dim_emb-40000_vocab/output-24.csv\n",
      "151592/151592 [==============================] - 2553s 17ms/step - loss: 0.0318 - acc: 0.9872 - val_loss: 0.0479 - val_acc: 0.9817\n",
      "Epoch 26/30\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9874-- CUSTOM METRICS --\n",
      "7979/7979 [==============================] - 14s 2ms/step\n",
      "roc-auc (test): 0.98170\n",
      "Significant: 775 (Actual) -- 715 (Predictions)\n",
      "-- CUSTOM METRICS END --\n",
      "153164/153164 [==============================] - 257s 2ms/step\n",
      "Wrote to file:  1_BiLstm_128-1_conv_128-Avg_Max_Pool-dense_128-200_dim_emb-40000_vocab/output-25.csv\n",
      "151592/151592 [==============================] - 2554s 17ms/step - loss: 0.0308 - acc: 0.9874 - val_loss: 0.0453 - val_acc: 0.9835\n",
      "Epoch 27/30\n",
      " 91648/151592 [=================>............] - ETA: 14:53 - loss: 0.0291 - acc: 0.9880"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-8a00fdadbc88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mKaggleOutputCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkaggle_test_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_SEQ_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     ],\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m )\n",
      "\u001b[0;32m~/Venv/mlpy3venv/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/Venv/mlpy3venv/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1207\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Venv/mlpy3venv/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Venv/mlpy3venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Venv/mlpy3venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Venv/mlpy3venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Venv/mlpy3venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Venv/mlpy3venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TEST_SPLIT = 0.05\n",
    "\n",
    "EMBEDDING_DIM = 200  # 50 / 100 / 200 / 300\n",
    "MAX_VOCAB_SIZE = 40000\n",
    "MAX_SEQ_LENGTH = 1400\n",
    "\n",
    "model_name_suffix = '-%d_dim_emb-%d_vocab' % (EMBEDDING_DIM, MAX_VOCAB_SIZE)\n",
    "\n",
    "datasetX_padded, tokenizer = process_dataset(datasetX_comments, MAX_VOCAB_SIZE, MAX_SEQ_LENGTH)\n",
    "embedding_layer = get_embedding_layer(tokenizer, EMBEDDING_DIM, MAX_VOCAB_SIZE, MAX_SEQ_LENGTH)\n",
    "\n",
    "trainX_padded, testX_padded, trainY, testY, trainX_comment, testX_comment = train_test_split(\n",
    "    datasetX_padded,\n",
    "    datasetY,\n",
    "    datasetX_comments,\n",
    "    test_size=TEST_SPLIT)\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQ_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = SpatialDropout1D(0.2)(embedded_sequences)\n",
    "x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "a = GlobalAveragePooling1D()(x)\n",
    "m = GlobalMaxPooling1D()(x)\n",
    "x = Concatenate()([a, m])\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "preds = Dense(6, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(sequence_input, preds, name='1_BiLstm_128-1_conv_128-Avg_Max_Pool-dense_128' + model_name_suffix)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    trainX_padded,\n",
    "    trainY,\n",
    "    validation_data=(testX_padded, testY),\n",
    "    epochs=30,\n",
    "    batch_size=128,\n",
    "    callbacks = [\n",
    "        CustomMetricsCallback(testX_padded, testY, freq=1),\n",
    "        PrintSamples(testX_padded, testX_comment, testY),\n",
    "        get_checkpoint_callback(model),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),\n",
    "        KaggleOutputCallback(tokenizer, kaggle_test_dataset, MAX_SEQ_LENGTH, freq=1)\n",
    "    ],\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Toxic comment classification challenge on Kaggle: [[Link]](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge).\n",
    "1. Using pre-trained word embeddings in a Keras model: [[Link]](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html).\n",
    "1. Guide To Multi-Class Multi-Label Classification With Neural Networks In Python: [[Link]](https://www.depends-on-the-definition.com/guide-to-multi-label-classification-with-neural-networks/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
